{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49cf34e-e85d-414d-85cf-e42fa55391d7",
   "metadata": {},
   "source": [
    "# ASF HyP3\n",
    "\n",
    "> Module for transferring HyP3 processed data to Earth Engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2fbf9-4e0b-4437-81fd-dffb36d06e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp asf_hyp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826cfbc-773c-4f0a-8f46-e390e54f63c6",
   "metadata": {},
   "source": [
    "# Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbb841-a470-4f65-9243-d8fda6ac788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datetime\n",
    "import logging\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import zipfile\n",
    "from pprint import pprint\n",
    "\n",
    "import asf_search\n",
    "from IPython.display import JSON\n",
    "import ee\n",
    "from fastcore.basics import patch\n",
    "import gcsfs\n",
    "from hyp3_sdk import HyP3\n",
    "from rio_cogeo import cogeo\n",
    "\n",
    "from sar_asf_to_gee import core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c3402-0888-4d11-be83-025d18802b26",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Authenticate to NASA Earthdata\n",
    "- See: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html\n",
    "- See: https://urs.earthdata.nasa.gov/documentation/for_users/data_access/create_net_rc_file\n",
    "\n",
    "Authenticate to Google Cloud\n",
    "- See: https://cloud.google.com/sdk/gcloud/reference/auth/application-default/loginca\n",
    "- `gcloud auth application-default login`\n",
    "\n",
    "Authenticate to Google Earth Engine\n",
    "- See: https://developers.google.com/earth-engine/guides/auth\n",
    "- `earthengine authenticate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc41f8-4e15-4cbf-a0fc-2555c4b97fb4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e5dd5-b75f-4268-9108-58582abe9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HyP3 instance.\n",
    "hyp3 = HyP3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36274a-cd96-4e3f-8ad9-50f903970150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level to display detailed information.\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f1889-9520-4eda-b9e8-53acfeeff6b5",
   "metadata": {},
   "source": [
    "# ASF HyP3 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28c1eb-72b0-4fb5-bd04-f7c1e272e450",
   "metadata": {},
   "source": [
    "[HyP3](https://hyp3-docs.asf.alaska.edu/) processing jobs can be initiated in a variety of ways, including the [Vertex](https://hyp3-docs.asf.alaska.edu/using/vertex/) web application and the [HyP3 Python SDK](https://hyp3-docs.asf.alaska.edu/using/sdk/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782b715-17cf-446f-a86e-6c21d6b41e66",
   "metadata": {},
   "source": [
    "## Starting HyP3 processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6c021-75fa-4334-85b0-235570180918",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = 'RTC_GAMMA'\n",
    "job_name = 'RTC_processing_example'\n",
    "granule_for_rtc = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8'\n",
    "def submit_job():\n",
    "    return hyp3.submit_rtc_job(granule_for_rtc, job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce8cb9-bc47-4bc9-b546-6503f9149841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_type = 'INSAR_GAMMA'\n",
    "# job_name = 'INSAR_GAMMA_processing_example'\n",
    "# granule1 = 'S1A_IW_SLC__1SDV_20240311T185926_20240311T185953_052938_066872_3CAD'\n",
    "# granule2 = 'S1A_IW_SLC__1SDV_20231206T185929_20231206T185956_051538_0638B3_78A8'\n",
    "# def submit_job():\n",
    "#     return hyp3.submit_insar_job(granule1, granule2, job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298d6bc-904d-4e9b-891a-1d125d3a113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_type = 'INSAR_ISCE_BURST'\n",
    "# job_name = 'INSAR_ISCE_BURST_processing_example'\n",
    "# burst1 = 'S1_184906_IW1_20240104T154111_VV_3C7F-BURST'\n",
    "# burst2 = 'S1_184906_IW1_20240116T154110_VV_D1E5-BURST'\n",
    "# def submit_job():\n",
    "#     return hyp3.submit_insar_isce_burst_job(burst1, burst2, job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144e6a2-ad27-40e7-8d28-5b84717c9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_completed = hyp3.find_jobs(\n",
    "    job_type=job_type,\n",
    "    name=job_name\n",
    ")\n",
    "print(f'Number of {job_type} jobs = {len(batch_completed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd8b6f-66dd-4bb6-b4db-e151ebc976a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_active = core.filter_jobs(batch_completed.jobs, expired=False)\n",
    "print(f'Number of active {job_type} jobs = {len(batch_active)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02926d84-d493-4934-8b45-b9081be3737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not batch_active:\n",
    "    print('Job results for {job_name} were not found. Starting a new job.')\n",
    "    job = submit_job()\n",
    "    #job = hyp3.watch(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e9c05-f722-462a-a2f7-72f3312c782c",
   "metadata": {},
   "source": [
    "## Finding HyP3 Files\n",
    "\n",
    "The status of previously submitted jobs can be checked on the following page:\n",
    "https://search.asf.alaska.edu/#/?searchType=On%20Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a055f18-9839-4a6c-8035-13e81ccb5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_succeeded = [job for job in batch_active \n",
    "                   if job.to_dict()['status_code'] == 'SUCCEEDED']\n",
    "if len(batch_succeeded) == 0:\n",
    "    print(f'No successful {job_type} jobs found. Please wait until one of the current active jobs finishes.')\n",
    "else:\n",
    "    print(f'Number of successful {job_type} jobs = {len(batch_succeeded)}')\n",
    "    print(f'Selecting the latest successful job.')\n",
    "    job = batch_active[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ef897-3d50-4f2d-9a5b-bd8e9e850d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(job.to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413bdfd-cd45-4981-a646-e998917df691",
   "metadata": {},
   "source": [
    "# Transfer completed jobs\n",
    "\n",
    "Create a class that can be used to transfer data between ASF HyP3, a local machine, Google Cloud Storage, and Earth Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c340d-916a-46c2-8abb-69118fdcd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transfer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        job_dict,  # HyP3 job dictionary \n",
    "        gcs_bucket,  # GCS bucket\n",
    "        gee_gcp_project, # GCP project used by Earth Engine\n",
    "        gee_image_collection=None,  # Name of the Earth Engine ImageCollection (optional)\n",
    "        local_storage=None,\n",
    "    ):\n",
    "        self.job_dict = job_dict\n",
    "        self.gcs_bucket = gcs_bucket\n",
    "        self.gee_gcp_project = gee_gcp_project\n",
    "        self.gee_image_collection = gee_image_collection\n",
    "        if local_storage:\n",
    "            self.tempdir = None\n",
    "            self.local_storage = local_storage\n",
    "        else:\n",
    "            self.tempdir = tempfile.TemporaryDirectory() \n",
    "            self.local_storage = self.tempdir.name\n",
    "            logging.debug(f'created temporary directory: {self.tempdir.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d686734-d895-4518-be87-dd4e22cf609b",
   "metadata": {},
   "source": [
    "Create an Transfer class instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad19d5-01a9-40e2-9115-fc3ca840c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Transfer(\n",
    "    job_dict=job.to_dict(),\n",
    "    gcs_bucket='hyp3-data-staging',\n",
    "    gee_gcp_project='sar-asf-to-gee',\n",
    "    gee_image_collection=f'HyP3-{job_name}',\n",
    "    local_storage='temp_downloads',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d1035-0177-4c8b-8671-0509af523fb9",
   "metadata": {},
   "source": [
    "Create a class method for transferring results from HyP3 to a local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe411cbe-99ac-4197-a914-4602c41a6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_local(\n",
    "    self:Transfer,           \n",
    "):\n",
    "    \"Transfer HyP3 results to local system, unzip, and update the job dictionary.\"    \n",
    "    logging.info(f'Starting hpy3_results_to_local()')\n",
    "    for file in self.job_dict['files']:\n",
    "        logging.info(f'Processing {file[\"filename\"]}')\n",
    "        asf_search.download_url(\n",
    "            url=file['url'],\n",
    "            path=self.local_storage,\n",
    "            filename=file['filename'],\n",
    "        )\n",
    "        # Unzip the file\n",
    "        logging.info(f'  Unzipping the file')\n",
    "        with zipfile.ZipFile(os.path.join(self.local_storage, file['filename']), 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.local_storage)\n",
    "\n",
    "        # List the TIF files.\n",
    "        scene_name = file['filename'].removesuffix('.zip')\n",
    "        tifs = [x for x in os.listdir(\n",
    "                    os.path.join('temp_downloads', scene_name))\n",
    "                if x.endswith('.tif')]\n",
    "\n",
    "        for count, tif in enumerate(tifs):\n",
    "            logging.info(f'  Converting to a Cloud Optimized GeoTIFF. {count + 1}/{len(tifs)}')\n",
    "            subprocess.run([\n",
    "                \"rio\",\n",
    "                \"cogeo\",\n",
    "                \"create\",\n",
    "                os.path.join(self.local_storage, scene_name, tif),\n",
    "                os.path.join(self.local_storage, scene_name, tif)\n",
    "            ])\n",
    "        \n",
    "        tif_dict = {}\n",
    "        pattern = rf'^({scene_name}_(.+).tif)$'\n",
    "        for i in tifs:\n",
    "            groups = re.search(pattern, i).groups()\n",
    "            tif_dict[groups[1]] = os.path.join(scene_name, groups[0])\n",
    "        \n",
    "        file['extracted'] = tif_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666679d-6f7f-4935-91ff-8ba8dc3f73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9715313-fed7-4821-851c-db9f684a8346",
   "metadata": {},
   "source": [
    "Display the job dictionary, which now includes the list of extracted files (`root` => `files` => # => `extracted`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a4086-69e8-40da-afe4-3edb5df12f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(t.job_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef4739-d762-484b-bfbc-423dfc3365d5",
   "metadata": {},
   "source": [
    "## Transfer to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01938b7b-ffb9-460f-ab12-863d9244616b",
   "metadata": {},
   "source": [
    "Create a method for transferring results from a local computer to Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33878c48-e085-4732-910f-cd2930778a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_gcs(\n",
    "    self:Transfer,\n",
    "):\n",
    "    logging.info('Starting to_gcs()')\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem(token='google_default')\n",
    "\n",
    "    for file in self.job_dict['files']:\n",
    "        for band, filename in file['extracted'].items():\n",
    "            gcs_path = f'{self.gcs_bucket}/{filename}'\n",
    "            if fs.exists(gcs_path):\n",
    "                logging.info(f'GCS file already exists:\\n    {gcs_path}')\n",
    "            else:\n",
    "                logging.info(f'Starting to transfer file to GCS:\\n    {gcs_path}')\n",
    "                # Transfer the local file to GCS.\n",
    "                fs.put_file(\n",
    "                    lpath=f\"{self.local_storage}/{filename}\",\n",
    "                    rpath=gcs_path\n",
    "                )    \n",
    "                logging.info(f'Transferred file to GCS: {gcs_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2210fb-af38-4074-bae3-4b0dbcd3b275",
   "metadata": {},
   "source": [
    "Transfer the files to Google Cloud Storage.  If your Google Cloud authentication credentials have expired, you will get an error and will need to reauthenticate\n",
    "`gcloud auth application-default login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a168f76-a382-4a21-8837-ec4c970ba166",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_gcs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af23f8a-a15e-416d-88fb-85956c394725",
   "metadata": {},
   "source": [
    "## Create a GEE Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad9a43-a49d-4ec8-815f-7160939210c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def create_gee_asset(\n",
    "    self:Transfer,\n",
    "):\n",
    "    \"Create an Earth Engine asset.\"\n",
    "    logging.info(f'Starting create_gee_asset()')\n",
    "    \n",
    "    ee.Initialize(project=self.gee_gcp_project)\n",
    "    \n",
    "    core.create_gee_image_collection(self.gee_gcp_project, self.gee_image_collection)\n",
    "\n",
    "    granule_names = self.job_dict['job_parameters']['granules']\n",
    "    granules = asf_search.granule_search(granule_names)\n",
    "\n",
    "    granule_times = [datetime.datetime.fromisoformat(x.properties['stopTime']) for x in granules]\n",
    "    start_time = min(granule_times)\n",
    "    end_time = max(granule_times)\n",
    "    \n",
    "    id = f\"{self.job_dict['job_id']}\"\n",
    "\n",
    "    props = granules[0].properties\n",
    "    description = (f\"{props['platform']}\"\n",
    "                   f\" - {props['processingLevel']}\"\n",
    "                   f\" - {props['beamModeType']}\")\n",
    "    \n",
    "    for file_dict in self.job_dict['files']:\n",
    "        for band, filename in file_dict['extracted'].items():\n",
    "\n",
    "            # Skip non-geocoded (native range-doppler coordinates) TIFFs.\n",
    "            if filename.endswith('_rdr.tif'):\n",
    "                continue\n",
    "            \n",
    "            gcs_path = f'{self.gcs_bucket}/{filename}'\n",
    "            \n",
    "            request = {\n",
    "                'type': 'IMAGE',\n",
    "                'bands': {  # TODO: Update this once multi-band COG assets are supported\n",
    "                    'id': band\n",
    "                },\n",
    "                'gcs_location': {\n",
    "                    'uris': [f'gs://{gcs_path}']\n",
    "                },\n",
    "                'properties': {\n",
    "                    'source':  file_dict['url'],\n",
    "                    'band': band  # TODO: Remove this once multi-band COG assets are supported\n",
    "                },\n",
    "                'startTime': start_time.strftime(core.FORMAT_GEE_DATETIME_STRING),\n",
    "                'endTime': end_time.strftime(core.FORMAT_GEE_DATETIME_STRING),\n",
    "                'description': description\n",
    "            }\n",
    "\n",
    "            path_parts = [\n",
    "                'projects',\n",
    "                self.gee_gcp_project,\n",
    "                'assets',\n",
    "                self.gee_image_collection,\n",
    "                # TODO: Remove the band suffix once multi-band COG assets are supported\n",
    "                f'{id}_{band}'.replace(\".\", \"_\") \n",
    "            ]\n",
    "            assetname = os.path.join(*[x for x in path_parts if x is not None])\n",
    "\n",
    "            logging.debug(f'request = {request}')\n",
    "            logging.debug(f'assetname = {assetname}')\n",
    "            try:\n",
    "                ee.data.createAsset(\n",
    "                    value=request,\n",
    "                    path=assetname\n",
    "                )  \n",
    "                logging.info(f'Finished creating a GEE asset:\\n    {assetname}.')\n",
    "            except ee.EEException as e:\n",
    "                print(f'e = {e}')\n",
    "                if \"does not exist or doesn't allow this operation\" in str(e):\n",
    "                    raise(e)\n",
    "                else:\n",
    "                    raise(e)  # TODO: Add logic to parse the EEException message.\n",
    "                    logging.info('GEE asset already exists. Skipping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3c84f-0c22-43b4-81d6-352cbcf2df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.create_gee_asset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d60fd-c24b-4d6f-b443-4ecb6bb64954",
   "metadata": {},
   "source": [
    "The Google Earth Engine Code Editor can be used to visualize these assets. Here is a template script that demonstrates basic visualization:\n",
    "https://code.earthengine.google.com/4140085702fc842227dc641426acb983\n",
    "Note that you will need to update the script to reference Earth Engine assets that you have permissions to access (for example: assets that you have created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229cc5c-4390-4ff6-96f5-9c6f2b3eadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16261c4b-6499-4674-8e51-df0b8bc862a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
